import copy
import numpy as np

from Layers.Base import BaseLayer


class NeuralNetwork:
    def __init__(self, optimizer=None):
        self.optimizer = optimizer
        self.loss = list()
        self.layers = list()
        self.data_layer = None
        self.loss_layer = None
        self._input_tensor = None
        self._label_tensor = None

    def _get_data(self) -> tuple:
        self._input_tensor, self._label_tensor = self.data_layer.next()
        return self._input_tensor, self._label_tensor

    def append_layer(self, layer:BaseLayer) -> None:    #'layer' obj of inherited sub-classes
        if layer.trainable:
            layer.optimizer = copy.deepcopy(self.optimizer)     #sets the optimization property for eligible layers
        self.layers.append(layer)       # creates self.layers with trainable as well as non-trainable layers


    def forward(self) -> float:
        input_tensor, self._label_tensor = self._get_data()

        for layer in self.layers:           # sums--> ReLU-->Sums--> does Softmax
            input_tensor = layer.forward(input_tensor)
        return self.loss_layer.forward(input_tensor, self._label_tensor)  # returns output of Entropy Loss layer


    def backward(self) -> None:
        error_tensor = self.loss_layer.backward(self._label_tensor)
        for layer in reversed(self.layers):
            error_tensor = layer.backward(error_tensor)

    def train(self, iterations) -> None:
        for itr in range(iterations):
            loss = self.forward()       #final loss generated by cross-entropy in one iteration
            self.loss.append(loss)
            self.backward()             #resets the tensor to input_tensor

    def test(self, input_tensor) -> np.ndarray:
        for layer in self.layers:
            input_tensor = layer.forward(input_tensor)
        return input_tensor